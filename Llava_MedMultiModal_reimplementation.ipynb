{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOubRqrPCyQ2lLPtoi/fEwS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yjianpen/MlePractice/blob/main/Llava_MedMultiModal_reimplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's download some data first!\n",
        "!git clone https://github.com/microsoft/LLaVA-Med.git\n"
      ],
      "metadata": {
        "id": "1C4cKMi-WlEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the directory and file exists\n",
        "\n",
        "!ls LLaVA-Med/data"
      ],
      "metadata": {
        "id": "F5OumFrgap1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Constants\n",
        "\n",
        "CONTROLLER_HEART_BEAT_EXPIRATION = 30\n",
        "WORKER_HEART_BEAT_INTERVAL = 15\n",
        "\n",
        "LOGDIR = \".\"\n",
        "\n",
        "# Model Constants\n",
        "IGNORE_INDEX = -100\n",
        "IMAGE_TOKEN_INDEX = -200\n",
        "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
        "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
        "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
        "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
        "IMAGE_PLACEHOLDER = \"<image-placeholder>\""
      ],
      "metadata": {
        "id": "IjSmhs2tfHQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
        "\n",
        "\n",
        "class CLIPVisionTower(nn.Module):\n",
        "    def __init__(self, vision_tower, args, delay_load=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.is_loaded = False\n",
        "\n",
        "        self.vision_tower_name = vision_tower\n",
        "        self.select_layer = args.mm_vision_select_layer\n",
        "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
        "\n",
        "        if not delay_load:\n",
        "            self.load_model()\n",
        "        else:\n",
        "            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n",
        "\n",
        "    def load_model(self):\n",
        "        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n",
        "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name)\n",
        "        self.vision_tower.requires_grad_(False)\n",
        "\n",
        "        self.is_loaded = True\n",
        "\n",
        "    def feature_select(self, image_forward_outs):\n",
        "        image_features = image_forward_outs.hidden_states[self.select_layer]\n",
        "        if self.select_feature == 'patch':\n",
        "            image_features = image_features[:, 1:]\n",
        "        elif self.select_feature == 'cls_patch':\n",
        "            image_features = image_features\n",
        "        else:\n",
        "            raise ValueError(f'Unexpected select feature: {self.select_feature}')\n",
        "        return image_features\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, images):\n",
        "        if type(images) is list:\n",
        "            image_features = []\n",
        "            for image in images:\n",
        "                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n",
        "                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n",
        "                image_features.append(image_feature)\n",
        "        else:\n",
        "            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n",
        "            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n",
        "\n",
        "        return image_features\n",
        "\n",
        "    @property\n",
        "    def dummy_feature(self):\n",
        "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.vision_tower.dtype\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.vision_tower.device\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        if self.is_loaded:\n",
        "            return self.vision_tower.config\n",
        "        else:\n",
        "            return self.cfg_only\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self):\n",
        "        return self.config.hidden_size\n",
        "\n",
        "    @property\n",
        "    def num_patches(self):\n",
        "        return (self.config.image_size // self.config.patch_size) ** 2"
      ],
      "metadata": {
        "id": "1kmWQhlTe0ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def build_vision_tower(vision_tower_cfg, **kwargs):\n",
        "    vision_tower = getattr(vision_tower_cfg, 'mm_vision_tower', getattr(vision_tower_cfg, 'vision_tower', None))\n",
        "    is_absolute_path_exists = os.path.exists(vision_tower)\n",
        "    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\"):\n",
        "        return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "\n",
        "\n",
        "class IdentityMap(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        return {\"mm_projector_type\": 'identity'}\n",
        "\n",
        "\n",
        "class SimpleResBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.pre_norm = nn.LayerNorm(channels)\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.pre_norm(x)\n",
        "        return x + self.proj(x)\n",
        "\n",
        "\n",
        "def build_vision_projector(config, delay_load=False, **kwargs):\n",
        "    projector_type = getattr(config, 'mm_projector_type', 'linear')\n",
        "\n",
        "    if projector_type == 'linear':\n",
        "        return nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
        "\n",
        "    mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n",
        "    if mlp_gelu_match:\n",
        "        mlp_depth = int(mlp_gelu_match.group(1))\n",
        "        modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n",
        "        for _ in range(1, mlp_depth):\n",
        "            modules.append(nn.GELU())\n",
        "            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n",
        "        return nn.Sequential(*modules)\n",
        "\n",
        "    if projector_type == 'identity':\n",
        "        return IdentityMap()\n",
        "\n",
        "    raise ValueError(f'Unknown projector type: {projector_type}')"
      ],
      "metadata": {
        "id": "RL1wHGaoeuBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#    Copyright 2023 Haotian Liu\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LlavaMetaModel:\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LlavaMetaModel, self).__init__(config)\n",
        "\n",
        "        if hasattr(config, \"mm_vision_tower\"):\n",
        "            self.vision_tower = build_vision_tower(config, delay_load=True)\n",
        "            self.mm_projector = build_vision_projector(config)\n",
        "\n",
        "    def get_vision_tower(self):\n",
        "        vision_tower = getattr(self, 'vision_tower', None)\n",
        "        if type(vision_tower) is list:\n",
        "            vision_tower = vision_tower[0]\n",
        "        return vision_tower\n",
        "\n",
        "    def initialize_vision_modules(self, model_args, fsdp=None, embed_tokens=None):\n",
        "        vision_tower = model_args.vision_tower\n",
        "        mm_vision_select_layer = model_args.mm_vision_select_layer\n",
        "        mm_vision_select_feature = model_args.mm_vision_select_feature\n",
        "        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
        "\n",
        "        self.config.mm_vision_tower = vision_tower\n",
        "\n",
        "        if self.get_vision_tower() is None:\n",
        "            vision_tower = build_vision_tower(model_args)\n",
        "\n",
        "            if fsdp is not None and len(fsdp) > 0:\n",
        "                self.vision_tower = [vision_tower]\n",
        "            else:\n",
        "                self.vision_tower = vision_tower\n",
        "        else:\n",
        "            if fsdp is not None and len(fsdp) > 0:\n",
        "                vision_tower = self.vision_tower[0]\n",
        "            else:\n",
        "                vision_tower = self.vision_tower\n",
        "            vision_tower.load_model()\n",
        "\n",
        "        self.config.use_mm_proj = True\n",
        "        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
        "        self.config.mm_hidden_size = vision_tower.hidden_size\n",
        "        self.config.mm_vision_select_layer = mm_vision_select_layer\n",
        "        self.config.mm_vision_select_feature = mm_vision_select_feature\n",
        "\n",
        "        # add additional configs for segtok\n",
        "        self.config.feature_outs = model_args.feature_outs\n",
        "        self.config.img_size = model_args.img_size\n",
        "        self.config.vision_backbone = model_args.vision_backbone\n",
        "        self.config.segtok_posembed = model_args.segtok_posembed\n",
        "\n",
        "        if getattr(self, 'mm_projector', None) is None:\n",
        "            self.mm_projector = build_vision_projector(self.config)\n",
        "        else:\n",
        "            # In case it is frozen by LoRA\n",
        "            for p in self.mm_projector.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "        # Initialize last layer in mm_projector with weight=0 and bias=mean(embed_tokens)\n",
        "        if embed_tokens is not None:\n",
        "            embed_tokens_weight = embed_tokens.weight.data\n",
        "            self.mm_projector[-1].weight.data.zero_()\n",
        "            self.mm_projector[-1].bias.data.copy_(embed_tokens_weight.mean(dim=0))\n",
        "\n",
        "        if pretrain_mm_mlp_adapter is not None:\n",
        "            def get_w(weights, keyword):\n",
        "                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n",
        "\n",
        "            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n",
        "            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n",
        "\n",
        "            # also load additional learnable parameters during feature alignment\n",
        "            checkpoint_folder = os.path.dirname(pretrain_mm_mlp_adapter)\n",
        "            ckpts = glob(f\"{checkpoint_folder}/checkpoint-*\", recursive = False)\n",
        "            if len(ckpts) > 0:\n",
        "                vision_module_weights = torch.load(f\"{ckpts[-1]}/mm_projector.bin\", map_location='cpu')\n",
        "                model_dict = get_w(vision_module_weights, 'vision_tower')\n",
        "                print(f\"Loading vision module weights from {ckpts[-1]}/mm_projector.bin\")\n",
        "                # print keys in model_dict\n",
        "                print(f\"Loaded keys: {model_dict.keys()}\")\n",
        "                self.vision_tower.load_state_dict(model_dict, strict=False)\n",
        "\n",
        "class LlavaMetaForCausalLM(ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_model(self):\n",
        "        pass\n",
        "\n",
        "    def get_vision_tower(self):\n",
        "        return self.get_model().get_vision_tower()\n",
        "\n",
        "    def encode_images(self, images):\n",
        "        image_features = self.get_model().get_vision_tower()(images)\n",
        "        image_features = self.get_model().mm_projector(image_features)\n",
        "        return image_features\n",
        "\n",
        "    def prepare_inputs_labels_for_multimodal(\n",
        "        self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None\n",
        "    ):\n",
        "        vision_tower = self.get_vision_tower()\n",
        "        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
        "            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\n",
        "                target_shape = past_key_values[-1][-1].shape[-2] + 1\n",
        "                attention_mask = torch.cat((attention_mask, torch.ones(\n",
        "                    (attention_mask.shape[0], target_shape - attention_mask.shape[1]),\n",
        "                    dtype=attention_mask.dtype,\n",
        "                    device=attention_mask.device\n",
        "                )), dim=1)\n",
        "                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n",
        "            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n",
        "\n",
        "        if type(images) is list or images.ndim == 5:\n",
        "            concat_images = torch.cat([image for image in images], dim=0)\n",
        "            image_features = self.encode_images(concat_images)\n",
        "            split_sizes = [image.shape[0] for image in images]\n",
        "            image_features = torch.split(image_features, split_sizes, dim=0)\n",
        "            image_features = [x.flatten(0, 1).to(self.device) for x in image_features]\n",
        "        else:\n",
        "            image_features = self.encode_images(images).to(self.device)\n",
        "\n",
        "        # TODO: image start / end is not implemented here to support pretraining.\n",
        "        if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # Let's just add dummy tensors if they do not exist,\n",
        "        # it is a headache to deal with None all the time.\n",
        "        # But it is not ideal, and if you have a better idea,\n",
        "        # please open an issue / submit a PR, thanks.\n",
        "        _labels = labels\n",
        "        _position_ids = position_ids\n",
        "        _attention_mask = attention_mask\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
        "        else:\n",
        "            attention_mask = attention_mask.bool()\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
        "\n",
        "        if labels is None:\n",
        "            labels = torch.full_like(input_ids, IGNORE_INDEX)\n",
        "\n",
        "        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
        "        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
        "\n",
        "        new_input_embeds = []\n",
        "        new_labels = []\n",
        "        cur_image_idx = 0\n",
        "        for batch_idx, cur_input_ids in enumerate(input_ids):\n",
        "            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
        "            if num_images == 0:\n",
        "                cur_image_features = image_features[cur_image_idx]\n",
        "                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n",
        "                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
        "                new_input_embeds.append(cur_input_embeds)\n",
        "                new_labels.append(labels[batch_idx])\n",
        "                cur_image_idx += 1\n",
        "                continue\n",
        "\n",
        "            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n",
        "            cur_input_ids_noim = []\n",
        "            cur_labels = labels[batch_idx]\n",
        "            cur_labels_noim = []\n",
        "            for i in range(len(image_token_indices) - 1):\n",
        "                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
        "                cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
        "\n",
        "            split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
        "            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n",
        "            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
        "            cur_new_input_embeds = []\n",
        "            cur_new_labels = []\n",
        "\n",
        "            for i in range(num_images + 1):\n",
        "                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
        "                cur_new_labels.append(cur_labels_noim[i])\n",
        "                if i < num_images:\n",
        "                    cur_image_features = image_features[cur_image_idx]\n",
        "                    cur_image_idx += 1\n",
        "                    cur_new_input_embeds.append(cur_image_features)\n",
        "                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
        "\n",
        "            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
        "            cur_new_labels = torch.cat(cur_new_labels)\n",
        "\n",
        "            new_input_embeds.append(cur_new_input_embeds)\n",
        "            new_labels.append(cur_new_labels)\n",
        "\n",
        "        # Truncate sequences to max length as image embeddings can make the sequence longer\n",
        "        tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
        "        if tokenizer_model_max_length is not None:\n",
        "            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
        "            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
        "\n",
        "        # Combine them\n",
        "        max_len = max(x.shape[0] for x in new_input_embeds)\n",
        "        batch_size = len(new_input_embeds)\n",
        "\n",
        "        new_input_embeds_padded = []\n",
        "        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
        "        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n",
        "\n",
        "        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
        "            cur_len = cur_new_embed.shape[0]\n",
        "            if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
        "                new_input_embeds_padded.append(torch.cat((\n",
        "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device),\n",
        "                    cur_new_embed\n",
        "                ), dim=0))\n",
        "                if cur_len > 0:\n",
        "                    new_labels_padded[i, -cur_len:] = cur_new_labels\n",
        "                    attention_mask[i, -cur_len:] = True\n",
        "                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
        "            else:\n",
        "                new_input_embeds_padded.append(torch.cat((\n",
        "                    cur_new_embed,\n",
        "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
        "                ), dim=0))\n",
        "                if cur_len > 0:\n",
        "                    new_labels_padded[i, :cur_len] = cur_new_labels\n",
        "                    attention_mask[i, :cur_len] = True\n",
        "                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
        "\n",
        "        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
        "\n",
        "        if _labels is None:\n",
        "            new_labels = None\n",
        "        else:\n",
        "            new_labels = new_labels_padded\n",
        "\n",
        "        if _attention_mask is None:\n",
        "            attention_mask = None\n",
        "        else:\n",
        "            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
        "\n",
        "        if _position_ids is None:\n",
        "            position_ids = None\n",
        "        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n",
        "\n",
        "    def initialize_vision_tokenizer(self, model_args, tokenizer):\n",
        "        if model_args.mm_use_im_patch_token:\n",
        "            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
        "            self.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        if model_args.mm_use_im_start_end:\n",
        "            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
        "            self.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "            if num_new_tokens > 0:\n",
        "                input_embeddings = self.get_input_embeddings().weight.data\n",
        "                output_embeddings = self.get_output_embeddings().weight.data\n",
        "\n",
        "                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
        "                    dim=0, keepdim=True)\n",
        "                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
        "                    dim=0, keepdim=True)\n",
        "\n",
        "                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "            if model_args.tune_mm_mlp_adapter:\n",
        "                for p in self.get_input_embeddings().parameters():\n",
        "                    p.requires_grad = True\n",
        "                for p in self.get_output_embeddings().parameters():\n",
        "                    p.requires_grad = False\n",
        "\n",
        "            if model_args.pretrain_mm_mlp_adapter:\n",
        "                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location='cpu')\n",
        "                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n",
        "                assert num_new_tokens == 2\n",
        "                if input_embeddings.shape == embed_tokens_weight.shape:\n",
        "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n",
        "                elif embed_tokens_weight.shape[0] == num_new_tokens:\n",
        "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n",
        "        elif model_args.mm_use_im_patch_token:\n",
        "            if model_args.tune_mm_mlp_adapter:\n",
        "                for p in self.get_input_embeddings().parameters():\n",
        "                    p.requires_grad = False\n",
        "                for p in self.get_output_embeddings().parameters():\n",
        "                    p.requires_grad = False"
      ],
      "metadata": {
        "id": "AuTmXqMme78v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1GygC0YfFfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
        "                         MistralConfig, MistralModel, MistralForCausalLM\n",
        "\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from transformers.generation.utils import GenerateOutput\n",
        "\n",
        "\n",
        "\n",
        "class LlavaMistralConfig(MistralConfig):\n",
        "    model_type = \"llava_mistral\"\n",
        "\n",
        "\n",
        "class LlavaMistralModel(LlavaMetaModel, MistralModel):\n",
        "    config_class = LlavaMistralConfig\n",
        "\n",
        "    def __init__(self, config: MistralConfig):\n",
        "        super(LlavaMistralModel, self).__init__(config)\n",
        "\n",
        "\n",
        "class LlavaMistralForCausalLM(MistralForCausalLM, LlavaMetaForCausalLM):\n",
        "    config_class = LlavaMistralConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(MistralForCausalLM, self).__init__(config)\n",
        "        self.model = LlavaMistralModel(config)\n",
        "\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        images: Optional[torch.FloatTensor] = None,\n",
        "        image_sizes: Optional[List[List[int]]] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            (\n",
        "                input_ids,\n",
        "                position_ids,\n",
        "                attention_mask,\n",
        "                past_key_values,\n",
        "                inputs_embeds,\n",
        "                labels\n",
        "            ) = self.prepare_inputs_labels_for_multimodal(\n",
        "                input_ids,\n",
        "                position_ids,\n",
        "                attention_mask,\n",
        "                past_key_values,\n",
        "                labels,\n",
        "                images,\n",
        "                image_sizes\n",
        "            )\n",
        "\n",
        "        return super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            labels=labels,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        images: Optional[torch.Tensor] = None,\n",
        "        image_sizes: Optional[torch.Tensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
        "        position_ids = kwargs.pop(\"position_ids\", None)\n",
        "        attention_mask = kwargs.pop(\"attention_mask\", None)\n",
        "        if \"inputs_embeds\" in kwargs:\n",
        "            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n",
        "\n",
        "        if images is not None:\n",
        "            (\n",
        "                inputs,\n",
        "                position_ids,\n",
        "                attention_mask,\n",
        "                _,\n",
        "                inputs_embeds,\n",
        "                _\n",
        "            ) = self.prepare_inputs_labels_for_multimodal(\n",
        "                inputs,\n",
        "                position_ids,\n",
        "                attention_mask,\n",
        "                None,\n",
        "                None,\n",
        "                images,\n",
        "                image_sizes=image_sizes\n",
        "            )\n",
        "        else:\n",
        "            inputs_embeds = self.get_model().embed_tokens(inputs)\n",
        "\n",
        "        return super().generate(\n",
        "            position_ids=position_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,\n",
        "                                      inputs_embeds=None, **kwargs):\n",
        "        images = kwargs.pop(\"images\", None)\n",
        "        image_sizes = kwargs.pop(\"image_sizes\", None)\n",
        "        inputs = super().prepare_inputs_for_generation(\n",
        "            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs\n",
        "        )\n",
        "        if images is not None:\n",
        "            inputs['images'] = images\n",
        "        if image_sizes is not None:\n",
        "            inputs['image_sizes'] = image_sizes\n",
        "        return inputs\n",
        "\n",
        "\n",
        "AutoConfig.register(\"llava_mistral\", LlavaMistralConfig)\n",
        "AutoModelForCausalLM.register(LlavaMistralConfig, LlavaMistralForCausalLM)"
      ],
      "metadata": {
        "id": "88-r5dvSa-z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXeWfPV6eo3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}